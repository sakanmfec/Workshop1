{"cells": [{"metadata": {"id": "99bd8be1-babf-478f-a618-8077411b9430"}, "cell_type": "markdown", "source": "<img align=\"left\" src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Assets&ArchHeader.jpeg?raw=true\">"}, {"metadata": {"id": "2517b27f-652a-48a4-8329-1850baa5d575"}, "cell_type": "markdown", "source": "# IBM Cloud Pak for Data Version 4.0.2 - Multi-Cloud Virtualization Hands-on Lab"}, {"metadata": {"id": "b9488e5e-815a-47a0-b278-cd203645fc20"}, "cell_type": "markdown", "source": "## Introduction\nWelcome to the IBM Cloud Pak for Data Multi-Cloud Virtualization Hands on Lab. \n\nIn this lab you analyze data from multiple data sources, from across multiple Clouds, without copying data into a warehouse.\n\nThis hands-on lab uses live databases, where data is \u201cvirtually\u201d available through the IBM Cloud Pak for Data Virtualization Service. This makes it easy to analyze data from across your multi-cloud enterprise using tools like, Jupyter Notebooks, Watson Studio or your favorite reporting tool like Cognos.  "}, {"metadata": {"id": "be5b0f97-e4d9-4eeb-be7d-be752b22f1c2"}, "cell_type": "markdown", "source": "### Where to find this sample online\nYou can find a copy of this notebook on GITHUB at https://github.com/Db2-DTE-POC/CPDDVHOL4."}, {"metadata": {"id": "bafbbeee-501f-49c2-acb6-f637c9f82313"}, "cell_type": "markdown", "source": "### The business problem and the landscape\nThe Acme Company needs timely analysis of stock trading data from multiple source systems. \n\nTheir data science and development teams needs access to:\n* Customer data\n* Account data\n* Trading data\n* Stock history and Symbol data\n\nThe data sources are running on premises and on the cloud. In this example many of the databases are also running on OpenShift but they could be managed, virtual or bare-metal cloud installations. IBM Cloud Pak for Data doesn't care. Enterprise DB (Postgres) is also running in the Cloud. Mongo and Informix are running on premises. \n\nTo simplify access for Data Scientists and Developers the Acme team wants to make all their data look like it is coming from a single database. They also want to combine data to create simple to use tables.\n\nIn the past, Acme built a dedicated data warehouse, and then created ETL (Export, Transform and Load) job to move data from each data source into the warehouse were it could be combined. Now they can just virtualize your data without moving it."}, {"metadata": {"id": "bbb799a1-7076-425e-a93e-5a715b32ddae"}, "cell_type": "markdown", "source": "### In this lab you learn how to:\n\n* Sign into IBM Cloud Pak for Data using your own Data Engineer userid\n* Connect to different data sources, on premises and across a multi-vendor Cloud\n* Make remote data from across your multi-vendor enterprise look and act like local tables in a single database\n* Make combining complex data and queries simple even for basic users\n* Capture complex SQL in easy to consume VIEWs that act just like simple tables\n* Ensure that users can securely access even complex data across multiple sources \n* Use roles and privileges to ensure that only the right user may see the right data\n* Make development easy by connecting to your virtualized data using Analytic tools and Application from outside of IBM Cloud Pak for Data. "}, {"metadata": {"id": "957a2882-231a-4847-8d7c-78868692c71d"}, "cell_type": "markdown", "source": "## Getting Started"}, {"metadata": {"id": "7f827827-d741-4144-ac5f-1c8f1cdffcc8"}, "cell_type": "markdown", "source": "### Using Jupyter notebooks\nYou are now officially using a Jupyter notebook! If this is your first time using a Jupyter notebook you might want to go through the Db2 Data Management Console Hands on Lab at www.ibm.biz/DMCDemosPOT. It includes an introduction to using Jupyter notebooks with the Db2 family. The introduction shows you some of the basics of using a notebook, including how to create the cells, run code, and save files for future use. \n\nJupyter notebooks are based on IPython which started in development in the 2006/7 timeframe. The existing Python interpreter was limited in functionality and work was started to create a richer development environment. By 2011 the development efforts resulted in IPython being released (http://blog.fperez.org/2012/01/ipython-notebook-historical.html).\n\nJupyter notebooks were a spinoff (2014) from the original IPython project. IPython continues to be the kernel that Jupyter runs on, but the notebooks are now a project on their own.\n\nJupyter notebooks run in a browser and communicate to the backend IPython server which renders this content. These notebooks are used extensively by data scientists and anyone wanting to document, plot, and execute their code in an interactive environment. The beauty of Jupyter notebooks is that you document what you do as you go along."}, {"metadata": {"id": "40593b27-5566-4d25-b0ff-16ef7ec7c092"}, "cell_type": "markdown", "source": "### Connecting to IBM Cloud Pak for Data\nFor this lab you will be assigned a Data Engineer userid. Check with the lab coordinator which userid you should use. Your user id will start with DATAENGINEER and end in a number from 1 to 20. (If you are using this for self-training you can use any number from 1 to 20.)\n\n* **Engineer:**\n    * ID: DATAENGINEERx\n    * PASSWORD: tsdvlab\n\nIf you have this notebook open, you should have already signed in as your assigned DATAENGINEER userid. \n1. To check your userid, click the icon at the very top right of the webpage. It will look something like this:\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/11.06.10 EngineerUserIcon.png?raw=true\">\n\n2. Click **Profile and settings** and review the user permissions for this user\n\nAs a Data Engineer you can:\n* Add and modify Data sources. Each source is a connection to a single database, either inside or outside of IBM Cloud Pak for Data.\n* Virtualize data. This makes tables in other data sources look and act like tables that are local to the Data Virtualization database\n* Work with the data you have virtualized.\n* Write SQL to access and join data that you have virtualized\n* See detailed information on how to connect external analytic tools and applications to your virtualized data\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/DataVirtualizationMainMenu.png?raw=true\">\n\nAs a User you can only:\n* Work with data that has been virtualized for you\n* Write SQL to work with that data\n* See detailed connection information\n\nAs an Administrator (only available to the course instructor) you can also:\n* Manage IBM Cloud Pak for Data User Access and Roles\n* Create and Manage Data Caches to accelerate performance\n* Change key service settings"}, {"metadata": {"id": "dc6dc212-2f7e-4277-950d-db94393a9251"}, "cell_type": "markdown", "source": "## Basic Data Virtualization"}, {"metadata": {"id": "ca17878a-2f46-49ef-b197-62f50af0fd12"}, "cell_type": "markdown", "source": "### Exploring Data Source Connections\nLet's start by looking at the Data Source Connections that are already available. \n\nYou should now have this Hands-on Lab notebook on the left side of your screen and the Cloud Pak for Data Console on the right side of your screen. In the Cloud Pak for Data Console:\n\n1. Click the three bar (hamburger) menu at the top left of the console\n2. Click on the Data menu item if is not already expanded\n3. Right click **Data virtualization** and select **Open in New Window**\n4. Arrange your windows so that notebook is on one side of your screen and the Cloud Pak Data Virtualization Console is on the other side. This makes it easier to follow the instructions without having to jump back and forth between the notebook and the console.\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/DesktopArrangment.png?raw=true\">\n5. If you don't see the page above, click the **Data Virtualization** menu in the Cloud Pak for Data Console and select **Data Sources**.\n4. Click **Constellation View**. A spider diagram of the connected data sources opens. \n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/ConstellationView.png?raw=true\">\n\n    This displays the Data Source Graph with 8 active data sources:\n    * 4 Db2 Family Databases hosted on premises, on Cloud Pak for Data and on OpenShift on AWS\n    * 1 Remote connector\n    * 1 MongoDB Enterprise data server running as a Cloud Pak for Data service and on Premises\n    * 1 Enterprise DB Postgres data server running on premises and on Cloud Pak for Data\n    * 1 Netezza Performance Server (using the Pure Data for Analyics connection) running on the Cloud\n    * 1 MySQL data server running on premises\n    * 1 Informix Database running on premises \n\n**You are not going to add a new data source**. However, you can go through the steps so you can see how to add additional data sources.\n1. Click **Add connection** in the upper-right of the console screen\n2. Select **Select existing connection** from the menu\nYou can see a history of other data source connection information that was used before. This history is maintained to make reconnecting to data sources easier and faster.\n3. Click **Data sources** at the top of the page to return to the Data Sources page.\nin the area below the title and above the list of existing connections.\n4. Click **Add connection** and **Create new connection**\n5. Scroll through all the **available data sources** to see the available connection types\n\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/ListOfAvailableTypes.png?raw=true\">\n\n6. Select a data source from the list to see the information required to connect to a new data source. \nAt a minimum you typically need the host URL and port address, database name, userid and password. You can also connect using an SSL certificate that can be dragged and dropped directly into the console interface. \n8. Click **Cancel** again to return to the list of currently connected data sources"}, {"metadata": {"id": "b4a433fa-c26b-4c46-8574-7cb3bb04b4ba"}, "cell_type": "markdown", "source": "### Exploring the available data\nNow that you understand how to connect to data sources you can start virtualizing data. Much of the work has already been done for you. IBM Cloud Pak for Data searches through the available data sources and compiles a single large inventory of all the tables and data available to virtualize in IBM Cloud Pak for Data. \n\n1. Click the Data Virtualization menu and select **Virtualize** under **Virtualization**\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/DataVirtualizationMainMenu.png?raw=true\">\n    \n2. Check the total number of available tables at the top of the list. There should be hundreds available.\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.24.37 PM.png?raw=true\">\n\n3. Enter \"STOCK\" into the search field and hit **Enter**. Any tables with the string\n**STOCK** in the table name, the table schema or with a column name that includes **STOCK** appears in the search results. \n\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2021-01-26 at 10.54.45 AM.png?raw=true\">\n\n4. Hover your mouse pointer to the far-right side to the search results table. A **preview** icon will appear on each row as you move your mouse. \n5. Click the **preview** icon beside one table. This displays a preview of the data in the selected table.\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/EyeIcon.png?raw=true\">\n\n6. Click **X** at the top right of the dialog box to return to the search results."}, {"metadata": {"id": "e6cc9625-7630-4238-bb12-6624236d0e7d"}, "cell_type": "markdown", "source": "### Creating New Tables on a Source Database\nSo that each user in this lab can have their own data to virtualize you will create your own table in a remote database.\n\nIn this part of the lab, you use this Jupyter notebook and Python code to connect to a source database, create a simple table and populate it with data. \n\nIBM Cloud Pak for Data will automatically detect the change in the source database and make the new table available for virtualization.\n\nIn this example, you connect to the Db2 database running in IBM Cloud Pak for Data but the database can be anywhere. All you need is the connection information and authorized credentials. \n\n   <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Db2CPDDatabase.png?raw=true\">"}, {"metadata": {"id": "768bf7f4-eb2f-494a-badd-79a8629e19f4"}, "cell_type": "markdown", "source": "The first step is to connect to one of our remote data sources directly as if we were part of the team building a new business application. Since each lab user will create their own table in their own schema the first thing you need to do is update and run the cell below with your engineer name. \n1. In this Juypyter notebook, click on the cell below \n2. **Update the lab number** in the cell below to your assigned user and lab number\n3. Click **Run** from the Jupyter notebook menu above"}, {"metadata": {"id": "8c926ff7-806f-4a28-a860-97a33cb8add6"}, "cell_type": "code", "source": "# Setting your userID\nlabnumber = 1\nengineer = 'DATAENGINEER' + str(labnumber)\nprint('variable engineer set to = ' + str(engineer))", "execution_count": null, "outputs": []}, {"metadata": {"id": "bf38b959-3056-42f2-9952-162996d4e4d0"}, "cell_type": "markdown", "source": "The next part of the lab relies on a Jupyter notebook extension, commonly refer to as a \"magic\" command, to connect to a Db2 database. To use the commands you load load the extension by running another notebook call db2 that contains all the required code \n<pre>\n&#37;run db2.ipynb\n</pre>\nThe cell below loads the Db2 extension directly from GITHUB. Note that it will take a few seconds for the extension to load, so you should generally wait until the \"Db2 Extensions Loaded\" message is displayed in your notebook. \n1. Click the cell below\n2. Click **Run**. When the cell is finished running, In[*] will change to In[2]"}, {"metadata": {"id": "f5f185a4-558c-4b67-af88-c2a828445ace"}, "cell_type": "code", "source": "!wget -O db2.ipynb https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVHOL4/main/db2.ipynb\n%run db2.ipynb\nprint('db2.ipynb loaded')", "execution_count": null, "outputs": []}, {"metadata": {"id": "fd32d8561b1344c280e69ab554c10e37"}, "cell_type": "markdown", "source": "#### Note on using Jupyter Notebooks\nIf you leave or if you restart your environment, or come back after a long absence to this notebook, you might see the following error when you try to run the **%sql** magic commands. \n\nIf you see the following error **UsageError: Line magic function '%sql' not found**, please rerun the cell above to re-load the db2 magic commands that support **%sql**."}, {"metadata": {"id": "ce970019-1b47-45b6-bf1f-3da2c4d5b016"}, "cell_type": "markdown", "source": "#### Connecting to Db2\n\nBefore any SQL commands can be issued, a connection needs to be made to the Db2 database that you will be using. \n\nThe Db2 magic command tracks whether or not a connection has occured in the past and saves this information between notebooks and sessions. When you start up a notebook and issue a command, the program will reconnect to the database using your credentials from the last session. In the event that you have not connected before, the system will prompt you for all the information it needs to connect. This information includes:\n\n- Database name\n- Hostname\n- PORT \n- Userid\n- Password"}, {"metadata": {"id": "f67b0176-6d3b-4e18-ac15-207875bcd78f"}, "cell_type": "markdown", "source": "#### Connecting to Db2"}, {"metadata": {"id": "47d5c4fa-7a27-403c-a866-ec09da9ab18e"}, "cell_type": "code", "source": "# Connect to the Db2 STOCKS database on IBM Cloud Pak for Data\ndatabase = 'STOCKS'\nuser = engineer\npassword = 'tsdvlab'\nhost = 'cpd-cpd-instance.apps.demo.ibmdte.net'\nport = '32443'\n\n%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}", "execution_count": null, "outputs": []}, {"metadata": {"id": "814c68f9-08f4-43a5-b14f-e40bb2101c5c"}, "cell_type": "markdown", "source": "To check that the connection is working. Run the following cell. It lists the tables in the database in the **DVDEMO** schema. Only the first 5 tables are listed."}, {"metadata": {"id": "8543e376-fc9e-41dc-807f-a63119b4d9e5"}, "cell_type": "code", "source": "%sql select TABNAME, TABSCHEMA, OWNER from syscat.tables where TABSCHEMA = 'STOCKS'", "execution_count": null, "outputs": []}, {"metadata": {"id": "bd94b9cf-f5d6-45b1-8497-344e44f6878a"}, "cell_type": "markdown", "source": "Now that you can successfully connect to the database, you are going to create two tables with the same name and column across two different schemas. In following steps of the lab you are going to virtualize these tables in IBM Cloud Paks for Data and fold them together into a single table. \n\nThe next cell sets the default schema to your engineer name followed by 'A'. Notice how you can set a python variable and substitute it into the SQL Statement in the cell. The **-e** option echos the command. \n\nRun the next cell."}, {"metadata": {"id": "475691ce-be35-49de-a3e0-c3c397fe94c1"}, "cell_type": "code", "source": "schema_name = engineer+'A'\ntable_name = 'DISCOVER'\n\nprint(\"\")\nprint(\"Lab #: \"+str(labnumber))\nprint(\"Schema name: \" + str(schema_name))\nprint(\"Table name: \" + str(table_name))\n\n%sql -e SET CURRENT SCHEMA {schema_name}", "execution_count": null, "outputs": []}, {"metadata": {"id": "e32ed550-7490-4a07-a57f-afc577672226"}, "cell_type": "markdown", "source": "Run next cell to create a table with a single INTEGER column containing values from 1 to 10. The **-q** flag in the %sql command supresses any warning message if the table already exists."}, {"metadata": {"id": "10670022-09df-4fcf-8cff-ae7fe10590a2"}, "cell_type": "code", "source": "sqlin = f'''\nDROP TABLE {table_name}; \nCREATE TABLE {table_name} (A INT); \nINSERT INTO {table_name} VALUES 1,2,3,4,5,6,7,8,9,10; \nSELECT * FROM {table_name}; \n'''\n\n%sql -q {sqlin}", "execution_count": null, "outputs": []}, {"metadata": {"id": "4a71e3a7-f85b-44eb-bc85-f740c3f84ea0"}, "cell_type": "markdown", "source": "Run the next two cells to create the same table in a schema ending in **B**. It is populated with values from 11 to 20."}, {"metadata": {"id": "fe369144-b2fe-4417-8ece-ca898b5106c8"}, "cell_type": "code", "source": "schema_name = engineer+'B'\ntable_name = 'DISCOVER'\n\nprint(\"\")\nprint(\"Lab #: \"+str(labnumber))\nprint(\"Schema name: \" + str(schema_name))\nprint(\"Table name: \" + str(table_name))\n\n%sql -e SET CURRENT SCHEMA {schema_name}", "execution_count": null, "outputs": []}, {"metadata": {"id": "efae00b3-edb9-4de5-b01c-85fdb1fe5bf9"}, "cell_type": "code", "source": "sqlin = f'''\nDROP TABLE {table_name}; \nCREATE TABLE {table_name} (A INT); \nINSERT INTO {table_name} VALUES 11,12,13,14,15,16,17,18,19,20; \nSELECT * FROM {table_name}; \n'''\n%sql -q {sqlin}", "execution_count": null, "outputs": []}, {"metadata": {"id": "a5c86b66-d410-4934-9888-43bd7de97e26"}, "cell_type": "markdown", "source": "Run the next cell to see all the tables in the database you just created. "}, {"metadata": {"id": "179831c5-cb4e-423e-a311-4524ff348a4c"}, "cell_type": "code", "source": "%sql SELECT TABSCHEMA, TABNAME FROM SYSCAT.TABLES WHERE TABSCHEMA LIKE '{engineer}%'", "execution_count": null, "outputs": []}, {"metadata": {"id": "a3a5acd1-b4e6-44be-9f2a-af91043d0b70"}, "cell_type": "markdown", "source": "Run the next cell to see all the tables in the database that are like **DISCOVER**. You may see tables created by other people running the lab. "}, {"metadata": {"id": "87ae9030-7b0d-4305-8325-2f8b703794b5"}, "cell_type": "code", "source": "%sql SELECT TABSCHEMA, TABNAME FROM SYSCAT.TABLES WHERE TABNAME LIKE 'DISCOVER%'", "execution_count": null, "outputs": []}, {"metadata": {"id": "5c4a895f-717a-405c-8f8c-5634332479d8"}, "cell_type": "markdown", "source": "### Virtualizing your new Tables\nNow that you have created two new tables you can virtualize that data and make it look like a single table in your database.\n1. Return to the IBM Cloud Pak for Data Console\n2. Click **Virtualize** in the Data Virtualization menu if you are not still in the Virtualize page\n3. Click the refresh icon to update the list of available tables. Data Virtualization caches the last search to avoid a long search through all your data source unless you need it.\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/RefreshIcon.png?raw=true\">\n3. Enter your current userid, for example DATAENGINEER1, in the search bar. (The search automatically looks for partial as well as full matches.) \n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize1.png?raw=true\">\n\n6. Select the two tables you just created by clicking the **check box** beside each table. Make sure you only select those for your DATAENGINEERnn schema. (Your table names will include the number of your lab participant number.)\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize2.png?raw=true\">\n\n7. Click **Add to Cart**. Notice that the number of items in your cart is now **2**.\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.29.37 PM.png?raw=true\">\n\n8. Click **View Cart**\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.29.43 PM.png?raw=true\">\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize3.png?raw=true\">\n    \n9. Change the name of your two tables from DISCOVER to **DISCOVERA** and **DISCOVERB**. These are the new names that you will be able to use to find your tables in the Data Virtualization database. Don't change the Schema name. It is unique to your current userid. \n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize4.png?raw=true\">\n\n10. Click **Back** upper right of the page. We are going to add one more thing to your cart.\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.31.25 PM.png?raw=true\">\n\n11. Click the gear icon at the upper-right of the page. \n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.31.44 PM.png?raw=true\">\n    \n12. Check the box beside **Group tables with identical names**. Notice how all the tables called **DISCOVER** have been grouped together into a single entry.\n    \n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.31.55 PM.png?raw=true\">    \n\n13. Select the row were all your DISCOVER tables have been grouped together\n14. Click **Add to cart**. \n15. Click **View cart**\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.33.09 PM.png?raw=true\">\n    \n    You should now see three items in your cart.\n16. Change the name of the new combined table to **DISCOVERFOLD**\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize5.png?raw=true\">\n\n17. Hover over the ellipsis icon at the right side of the list for the **DISCOVER** table\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.33.46 PM.png?raw=true\">\n\n18. Select **Edit grouped tables**\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.34.02 PM.png?raw=true\">\n\n19. Deselect all the tables except for those in one of the schemas you created. You should now have two tables selected. \n20. Click **Apply**\n21. Select **My Virtualized Data**. \n22. Click **Virtualize**. You see that three new virtual tables have been created. \n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.34.39 PM.png?raw=true\">\n    \n    The Virtual tables created dialog box opens.\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize6.png?raw=true\">\n     \n23. Click **View my virtualized data**. You return to the My virtualized data page."}, {"metadata": {"id": "3971592c-d5ef-408d-a46a-3a699328bd02"}, "cell_type": "markdown", "source": "### Working with your new tables\n1. Enter DISCOVER\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.35.31 PM.png?raw=true\">\n    \nYou should see the three virtual tables you just created. Notice that you do not see tables that other users have created. By default, Data Engineers only see virtualized tables they have virtualized or virtual tables where they have been given access by other users. \n2. Click the elipsis (...) beside your **DISCOVERFOLD** table and select **Preview** to confirm that it contains 20 rows.\n3. Click **Run SQL** from the Data Virtualization menu\n\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/DataVirtualizationMainMenu.png?raw=true\">\n\n4. Enter **SELECT * FROM DISCOVERFOLD ORDER BY A;** into the SQL Editor\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.38.06 PM.png?raw=true\">\n\n5. Click **Run All** at the bottom left of the SQL Editor window. \n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.36.45 PM.png?raw=true\">\n    \n6. Review the 20 rows returned in the result. Click **More** to see all the rows. The rows from both tables are combined into this new table.\n\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.38.02 PM.png?raw=true\">\n\nNotice that you didn't have to specify the schema for your new virtual tables. The SQL Editor automatically uses the schema associated with your userid that was used when you created your new tables. \n\nNow you can:\n* Create connection to a remote data source \n* Make a new or existing table in that remote data source look and act like a local table \n* Fold data from different tables in the same data source or access data sources by folding it together into a single virtual table"}, {"metadata": {"id": "d847b08c-104f-4882-935a-0cbf34a3ec3e"}, "cell_type": "markdown", "source": "## Gaining Insight from Virtualized Data"}, {"metadata": {"id": "43e39107-4cf2-42c9-b469-0a42bfd3edda"}, "cell_type": "markdown", "source": "Now that you understand the basics of Data Virtualization you can explore how easy it is to gain insight across multiple data sources without moving data. \n\nIn the next set of steps, you connect to virtualized data from this notebook using your DATAENGINEER userid. You can use the same techniques to connect to virtualized data from applications and analytic tools from outside of IBM Cloud Pak for Data. \n\nConnecting to all your virtualized data is just like connecting to a single database. All the complexity of dozens of tables across multiple databases on different on premises and cloud providers is now as simple as connecting to a single database and querying a table. \n\nWe are going to connect to the IBM Cloud Pak for Data Virtualization database in exactly the same way we connected to a Db2 database earlier in this lab. However we need to change the detailed connection information in the next notebook cell.\n\nSomething new for Cloud Pak for Data Version 3.5: The user id and password that you use to log into the Cloud Pak for Data console are the same one that you use to log into the data virtualization service through an application. This greatly simplifies user management. "}, {"metadata": {"id": "7a21f500-7602-462e-bcb8-9d82a6b47b1f"}, "cell_type": "markdown", "source": "#### Connecting to Data Virtualization SQL Engine\nRun the cell below. Your DATAENGINEER username is automatically used. "}, {"metadata": {"id": "36496e94-8a6d-4073-8104-91c7ca22b623"}, "cell_type": "code", "source": "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\ndatabase = 'bigsql'\nuser = engineer\npassword = 'tsdvlab'\nhost = 'cpd-cpd-instance.apps.demo.ibmdte.net'\nport = '31193'\n\n%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}", "execution_count": null, "outputs": []}, {"metadata": {"id": "b7b5bff22d4b4aa88830e13538f38bb1"}, "cell_type": "markdown", "source": "Now that you are connected to the Data Virtualization engine you can query the virtualized tables using all the power in the Db2 SQL query engine. \n\nThere are five tables that are available in each of the data sources. The examples below use a variety of data sources. For simplicity the schema of each data source represents where the data comes from. For example the NETEZZA.STOCK_SYMBOLS table is a virtual table that retrieves it data from a NETEZZA database. For your own projects you can choose any schema. There is no requirement to choose a schema that represents the source. "}, {"metadata": {"id": "7e78fbd03adf4b058a9d6ff3c3e0ea52"}, "cell_type": "markdown", "source": "The first table is a list of customer accounts with the current total number of stock trading transactions and the current account balance."}, {"metadata": {"id": "2625f169aee846e899917c44f0fcc3dc"}, "cell_type": "code", "source": "%sql -a select * from NETEZZA.STOCK_SYMBOLS ORDER BY SYMBOL FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "be25578215be470eabdb256ae6e56607"}, "cell_type": "markdown", "source": "The same table is also available from a different source. In this case the STOCK_SYMBOLS table from a virtualized XLSX file is available as XLSX.STOCK_SYMBOLS."}, {"metadata": {"id": "ecb42360447b4ec78c8928a01ab0d288"}, "cell_type": "code", "source": "%sql -a select * from XLSX.STOCK_SYMBOLS ORDER BY SYMBOL FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "9de2c837e13a4b9a86680ebc8e62744e"}, "cell_type": "markdown", "source": "To keep things simple, the virtualized data sources used in these examples have been encapsulated in views each using the TRADING schema. For example, the SQL below was used to map each virtualized table to a view. So you can create SQL to query the tables using the TRADING schema for each table. Later you can drop and recreate a view using a different source and keep the SQL you will run using the TRADING schema. \n\n    CREATE VIEW TRADING.CUSTOMER AS SELECT * FROM DB2OLTPONCPD.CUSTOMER;\n    CREATE VIEW TRADING.PORTFOLIO AS SELECT * FROM XLSX.PORTFOLIO;\n    CREATE VIEW TRADING.STOCK_SYMBOLS AS SELECT COMPANY, SYMBOL FROM NETEZZA.STOCK_SYMBOLS;\n    CREATE VIEW TRADING.STOCK_HISTORY AS SELECT * FROM EDBSTOCKSONPREMISES.STOCK_HISTORY;\n    CREATE VIEW TRADING.STOCK_TRANSACTIONS AS SELECT * FROM DB2AWS.STOCK_TRANSACTIONS;\n    CREATE VIEW TRADING.ACCOUNTS AS SELECT * FROM MYSQL.ACCOUNTS;\n"}, {"metadata": {"id": "7defc701dea64f358eeb03e2fde3837b"}, "cell_type": "markdown", "source": "Now, let's explore some of the queries that we can run to gain insight from these tables. Have a look at the structure of each of the five tables. The represent trading information for a set of customers for a stock brokerage firm. "}, {"metadata": {"id": "95f9ed14414a426c82577e92be1e1d4b"}, "cell_type": "code", "source": "%sql -a select * from TRADING.CUSTOMER FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "2fadd292c1a840dab2c7ed8da509e86d"}, "cell_type": "code", "source": "%sql -a select * from TRADING.PORTFOLIO FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "41cec17bd2e9413e9778f2f728731a62"}, "cell_type": "code", "source": "%sql -a select * from TRADING.STOCK_SYMBOLS FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "87a1b626d53243bda8aaee349371a8c3"}, "cell_type": "code", "source": "%sql -a select * from TRADING.STOCK_HISTORY FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "40ab05c6bdad425ea3d851a706fa6474"}, "cell_type": "code", "source": "%sql -a select * from TRADING.STOCK_TRANSACTIONS FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "2bb6bdd9-6856-4336-ba67-3f015ed53107"}, "cell_type": "code", "source": "%sql -a select * from TRADING.ACCOUNTS FETCH FIRST 10 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "02f9df80-36f6-40a6-9798-5b5689a22839"}, "cell_type": "markdown", "source": "### Building Insight from the Virtualized Tables\nOnce you have tables virtualized you can use all the power of SQL and the Db2 SQL Engine to build complex and rich queries of your data no mater where it was originally located.\n#### Get Price of a Stock over the Year\nSet the Stock Symbol in the line below and run the cell. This information is folded together with data coming from two identical tables, one on Db2 database and on on and Informix database. Run the next two cells. Then pick a new stock symbol from the list above, enter it into the cell below and run both cells again.\n\n**Source Cloud Pak for Data - Db2 Warehouse**"}, {"metadata": {"id": "20c833e5-2f9b-4fd1-bc46-d9a4ed725b65"}, "cell_type": "code", "source": "stock = 'AXP'\nprint('variable stock set to = ' + str(stock))", "execution_count": null, "outputs": []}, {"metadata": {"id": "ca884c4d-39a1-4fde-8849-a90873a1a34f"}, "cell_type": "code", "source": "%%sql -pl\nSELECT WEEK(TX_DATE) AS WEEK, OPEN FROM TRADING.STOCK_HISTORY\nWHERE SYMBOL = :stock AND TX_DATE != '2017-12-01'\nORDER BY WEEK(TX_DATE) ASC", "execution_count": null, "outputs": []}, {"metadata": {"id": "f86739d9-5c33-43a2-a44b-53732da4c4e8"}, "cell_type": "markdown", "source": "#### Trend of Three Stocks\nThis chart shows three stock prices over the course of a year. It uses the same folded stock history information."}, {"metadata": {"id": "d25ae779-2897-488c-8ef6-a48481a0ecd2"}, "cell_type": "code", "source": "stocks = ['INTC','MSFT','AAPL']", "execution_count": null, "outputs": []}, {"metadata": {"id": "0184c0b8-ef98-4dc7-8347-8cb352b49a62"}, "cell_type": "code", "source": "%%sql -pl\nSELECT SYMBOL, WEEK(TX_DATE), OPEN FROM TRADING.STOCK_HISTORY\nWHERE SYMBOL IN (:stocks) AND TX_DATE != '2017-12-01'\nORDER BY WEEK(TX_DATE) ASC", "execution_count": null, "outputs": []}, {"metadata": {"id": "d8cfe7b6-11e9-4c3e-a3c7-8302bf3119ce"}, "cell_type": "markdown", "source": "#### 30 Day Moving Average of a Stock\nEnter the Stock Symbol below to see the 30 day moving average of a single stock."}, {"metadata": {"id": "2c6f7c5c-1541-42d9-9180-9582cfb1e682"}, "cell_type": "code", "source": "stock = 'AAPL'", "execution_count": null, "outputs": []}, {"metadata": {"id": "efa8dcd2-275c-43d2-8b25-b72475408276"}, "cell_type": "code", "source": "sqlin = \\\n\"\"\"\nSELECT WEEK(TX_DATE) AS WEEK, OPEN, \n     AVG(OPEN) OVER (\n       ORDER BY TX_DATE\n     ROWS BETWEEN 15 PRECEDING AND 15 FOLLOWING) AS MOVING_AVG\n  FROM TRADING.STOCK_HISTORY\n     WHERE SYMBOL = :stock\n  ORDER BY WEEK(TX_DATE)\n\"\"\"\ndf = %sql {sqlin}\ntxdate= df['WEEK']\nsales = df['OPEN']\navg = df['MOVING_AVG']\n\nplt.xlabel(\"Day\", fontsize=12);\nplt.ylabel(\"Opening Price\", fontsize=12);\nplt.suptitle(\"Opening Price and Moving Average of \" + stock, fontsize=20);\nplt.plot(txdate, sales, 'r');\nplt.plot(txdate, avg, 'b');\nplt.show();", "execution_count": null, "outputs": []}, {"metadata": {"id": "b5c75afe-d4af-492b-804c-b5fe0318b3b7"}, "cell_type": "markdown", "source": "#### Trading volume of INTC versus MSFT and AAPL in first week of November\nEnter three stock symbols below."}, {"metadata": {"id": "a042c801-f88c-4d9f-8865-449208dd6d0a"}, "cell_type": "code", "source": "stocks = ['INTC','MSFT','AAPL']", "execution_count": null, "outputs": []}, {"metadata": {"id": "cedb5123-f6b0-4bc3-ae7f-882a1ce7e0a3"}, "cell_type": "code", "source": "%%sql -pb\nSELECT SYMBOL, DAY(TX_DATE), VOLUME/1000000 FROM TRADING.STOCK_HISTORY\nWHERE SYMBOL IN (:stocks) AND WEEK(TX_DATE) =  45\nORDER BY DAY(TX_DATE) ASC", "execution_count": null, "outputs": []}, {"metadata": {"id": "8ee32b6d-f7df-4823-8e70-2caf01ff0f94"}, "cell_type": "markdown", "source": "#### Show Stocks that Represent at least 3% of the Total Purchases during Week 45"}, {"metadata": {"id": "7fada678-59df-4a97-8789-e72d4aa76bf8"}, "cell_type": "code", "source": "%%sql -pie\nWITH WEEK45(SYMBOL, PURCHASES) AS (\n  SELECT SYMBOL, SUM(VOLUME * CLOSE) FROM TRADING.STOCK_HISTORY\n    WHERE WEEK(TX_DATE) =  45 AND SYMBOL <> 'DJIA'\n  GROUP BY SYMBOL\n),\nALL45(TOTAL) AS (\n  SELECT SUM(PURCHASES) * .03 FROM WEEK45\n)\nSELECT SYMBOL, PURCHASES FROM WEEK45, ALL45\nWHERE PURCHASES > TOTAL\nORDER BY SYMBOL, PURCHASES", "execution_count": null, "outputs": []}, {"metadata": {"id": "00e21b91-8d9e-4aa5-bb96-89210b657736"}, "cell_type": "markdown", "source": "### Stock Transaction Table\n#### Show Transactions by Customer\n\n**Source EDB Postgres on Cloud Pak for Data**"}, {"metadata": {"id": "00f614e6736c43b38578e401da984287"}, "cell_type": "code", "source": "%sql -a select * from TRADING.STOCK_TRANSACTIONS WHERE CUSTID ='101804' FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "021ef4b7-4494-40cb-9bb2-997820ade117"}, "cell_type": "markdown", "source": "#### Bought/Sold Amounts of Top 5 stocks "}, {"metadata": {"id": "49558fcb-b19a-48f0-8b06-dfe73425d2b2"}, "cell_type": "code", "source": "%%sql -a\nWITH BOUGHT(SYMBOL, AMOUNT) AS\n  (\n  SELECT SYMBOL, SUM(QUANTITY) FROM TRADING.STOCK_TRANSACTIONS\n  WHERE QUANTITY > 0\n  GROUP BY SYMBOL\n  ),\nSOLD(SYMBOL, AMOUNT) AS\n  (\n  SELECT SYMBOL, -SUM(QUANTITY) FROM TRADING.STOCK_TRANSACTIONS\n  WHERE QUANTITY < 0\n  GROUP BY SYMBOL\n  )\nSELECT B.SYMBOL, B.AMOUNT AS BOUGHT, S.AMOUNT AS SOLD\nFROM BOUGHT B, SOLD S\nWHERE B.SYMBOL = S.SYMBOL\nORDER BY B.AMOUNT DESC\nFETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "c441f9e6-3ed3-4a3d-b262-4d2610724030"}, "cell_type": "markdown", "source": "### Customer Accounts\n#### Show Top 5 Customer Balance\nThese next two examples use data from an Informix database\n\n**Source - Informix on Premises**"}, {"metadata": {"id": "b0935077-2bf9-4fd7-b241-e70ece0be987"}, "cell_type": "code", "source": "%%sql -a\nSELECT CUSTID, BALANCE FROM TRADING.ACCOUNTS\nORDER BY BALANCE DESC\nFETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "44b57c79-45c5-408d-958b-1b1fc9a9600c"}, "cell_type": "markdown", "source": "#### Show Bottom 5 Customer Balance\n**AWS - Db2, Azure - Postgres, Azure - Db2**"}, {"metadata": {"id": "da6426c9-9dc0-4f18-aed1-2e5938831e9a"}, "cell_type": "code", "source": "%%sql -a\nSELECT CUSTID, BALANCE FROM TRADING.ACCOUNTS\nORDER BY BALANCE ASC\nFETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "045e094c-dc78-4550-b7da-622b2cb234bd"}, "cell_type": "markdown", "source": "### Selecting Customer Information from MongoDB\nThe MongoDB database (running on premises) has customer information in a document format. In order to materialize the document data as relational tables, a total of four virtual tables are generated by the Data Virtualization engine. The following query shows the tables that are generated for the Customer document collection and how to join them into a single relational table."}, {"metadata": {"id": "de44d576-da63-4629-afce-2e07e3fe6cf6"}, "cell_type": "code", "source": "%sql -a select TABSCHEMA, TABNAME, COLCOUNT from syscat.tables where TABSCHEMA = 'MONGOONCPD' and TABNAME like 'CUSTOMER%'", "execution_count": null, "outputs": []}, {"metadata": {"id": "00100aa0-cc89-422e-9167-eae9a6574281"}, "cell_type": "markdown", "source": "The tables are all connected through the CUSTOMERID field, which is based on the generated _id of the main CUSTOMER colllection. In order to reassemble these tables into a document, we must join them using this unique identifier. An example of the contents of the CUSTOMER_CONTACT table is shown below."}, {"metadata": {"id": "38e57e56-c5bd-4d59-bee2-0f9586fd8da9"}, "cell_type": "code", "source": "%sql -a SELECT * FROM MONGOONCPD.CUSTOMER_CONTACT FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "4acc7f9b-cd8f-460e-8dd2-26de34904f14"}, "cell_type": "markdown", "source": "A full document record is shown in the following SQL statement which joins all of the tables together."}, {"metadata": {"id": "b6f5dabd-7202-44f3-9563-39eaef4aee79"}, "cell_type": "code", "source": "%%sql -a\nSELECT C.CUSTOMERID AS CUSTID, \n       CI.FIRSTNAME, CI.LASTNAME, CI.BIRTHDATE,\n       CC.CITY, CC.ZIPCODE, CC.EMAIL, CC.PHONE, CC.STREET, CC.STATE,\n       CP.CARD_TYPE, CP.CARD_NO\nFROM MONGOONCPD.CUSTOMER C, MONGOONCPD.CUSTOMER_CONTACT CC, \n     MONGOONCPD.CUSTOMER_IDENTITY CI, MONGOONCPD.CUSTOMER_PAYMENT CP\nWHERE  CC.CUSTOMER_ID = C.\"_ID\" AND\n       CI.CUSTOMER_ID = C.\"_ID\" AND\n       CP.CUSTOMER_ID = C.\"_ID\"\nFETCH FIRST 3 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "c2192437-a155-4514-aa66-2ec6e3821a0b"}, "cell_type": "markdown", "source": "### Joining Virtualized Data\nIn this final example we use data from four different data sources to answer a complex business question. \"What are the names of the customers in Ohio, who bought the most during the highest trading day of the year (based on the Dow Jones Industrial Index)?\" \n\n**Data Sources: Db2 Warehouse, MongoDB, Enterprise Postgres**"}, {"metadata": {"id": "6aa432f4-03da-4a55-9502-288c2e6adb85"}, "cell_type": "code", "source": "%%sql -a\nWITH MAX_VOLUME(AMOUNT) AS (\n  SELECT MAX(VOLUME) FROM TRADING.STOCK_HISTORY\n    WHERE SYMBOL = 'DJIA'\n),\nHIGHDATE(TX_DATE) AS (\n  SELECT TX_DATE FROM TRADING.STOCK_HISTORY, MAX_VOLUME M\n    WHERE SYMBOL = 'DJIA' AND VOLUME = M.AMOUNT\n),\nCUSTOMERS_IN_OHIO(CUSTID, LASTNAME) AS (\n  SELECT C.CUSTOMERID, CI.LASTNAME\n    FROM  MONGOONCPD.CUSTOMER C, \n          MONGOONCPD.CUSTOMER_CONTACT CC,\n          MONGOONCPD.CUSTOMER_IDENTITY CI\n    WHERE CC.CUSTOMER_ID = C.\"_ID\" AND\n          CI.CUSTOMER_ID = C.\"_ID\" AND\n          CC.STATE = 'OH'\n),\nTOTAL_BUY(CUSTID,TOTAL) AS (\n  SELECT C.CUSTID, SUM(SH.QUANTITY * SH.PRICE) \n    FROM CUSTOMERS_IN_OHIO C, TRADING.STOCK_TRANSACTIONS SH, HIGHDATE HD\n  WHERE SH.CUSTID = C.CUSTID AND\n        SH.TX_DATE = HD.TX_DATE AND \n        SH.QUANTITY > 0 \n  GROUP BY C.CUSTID\n)\nSELECT C.LASTNAME, T.TOTAL \n  FROM CUSTOMERS_IN_OHIO C, TOTAL_BUY T\nWHERE C.CUSTID = T.CUSTID\nORDER BY TOTAL DESC\nFETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "bb83524c-020d-4945-9a73-00c6ed1e17c3"}, "cell_type": "markdown", "source": "### Seeing where your Virtualized Data is coming from\nYou may eventually work with a complex Data Virtualization schema with dozens or hundreds of data sources. As an administrator or a Data Scientist you may need to understand where data is coming from. \n\nFortunately, the Data Virtualization engine is based on Db2. It includes the same catalog of information as does a Db2 database with some additional features. If you want to work backwards and understand where each of your virtualized tables comes from. The list of virtualized tables is included in the **SYSCAT.NICKNAMES** catalog table. \n\nRun the following SQL to see all the virtual tables in your Data Virtualization system. We exclude the **DVSYS** schema since it contains system created virtual tables (nicknames), not user created virtual tables. "}, {"metadata": {"id": "16738aff-a3a7-4372-8bc0-d269b77abaeb"}, "cell_type": "code", "source": "%%sql -a\nSELECT TABSCHEMA, TABNAME\n  FROM SYSCAT.NICKNAMES\n    WHERE TABSCHEMA != 'DVSYS'\n    ORDER BY TABSCHEMA, TABNAME", "execution_count": null, "outputs": []}, {"metadata": {"id": "1624ed1bf4b54d0593195b5fa89aa640"}, "cell_type": "markdown", "source": "If you want to see exactly where a virtualized table comes from you can substitute the the schema and virtual table name in the following SQL procedure. For example the next cell retrieves the location of the NETEZZA.STOCK_SYMBOLS table. You can see the source schema name the source table name as well as the connection information."}, {"metadata": {"id": "13141fa98bea43c39fa29a561f7d0410"}, "cell_type": "code", "source": "%%sql -a \nselect * from table(dvsys.GET_VT_SOURCES('NETEZZA', 'STOCK_SYMBOLS'))", "execution_count": null, "outputs": []}, {"metadata": {"id": "97f90eb64a48425f8f7a268948b9701f"}, "cell_type": "markdown", "source": "To see the source of all your virtual tables we just need to join the query and the procedure call."}, {"metadata": {"id": "7e48a389dd01418594203c422f3adf75"}, "cell_type": "code", "source": "%%sql -a\nSELECT N.TABSCHEMA AS TABSCHEMA, N.TABNAME AS TABNAME, S.SRCTABNAME AS SRCTABNAME, S.SRCSCHEMA AS SRCSCHEMA, S.SRCTYPE AS TYPE, S.DRIVER AS DRIVER, S.URL AS URL, S.USER AS USER, S.HOSTNAME AS HOSTNAME, S.PORT AS PORT, S.DBNAME AS DBNAME\n  FROM SYSCAT.NICKNAMES N, TABLE(\n  DVSYS.GET_VT_SOURCES(N.TABSCHEMA, N.TABNAME)) S\n  WHERE N.TABSCHEMA != 'DVSYS'", "execution_count": null, "outputs": []}, {"metadata": {"id": "68881735-b27d-47f7-9a86-cf5869decd5d"}, "cell_type": "markdown", "source": "As part of this lab, a view had already been created in the DV engine that encapsulates the query above. It is the **ADMIN.REMOTETABLESOURCE** view. Run the following SQL to see how it works. Notice that you can use it just like any table and qualify the results with a where or order by clause. In this example we are listing all the virtual tables that come from a Netezza source server.\n\nAccess has been granted to all users to this view. But you can choose to limit who can use it by granting or revoking permissions to it."}, {"metadata": {"id": "e20ffc633a5445a7af8c853b736245f8"}, "cell_type": "code", "source": "%%sql -a \nSELECT TABSCHEMA, TABNAME \n    FROM ADMIN.REMOTETABLESOURCE \n    WHERE TYPE = 'Netezza'\n    ORDER BY 'TABSCHEMA', 'TABNAME'", "execution_count": null, "outputs": []}, {"metadata": {"id": "d1edbbf86065441f8ebba99d3d6b7802"}, "cell_type": "markdown", "source": "You can also find the which tables a view is dependent on by querying the SYSCAT.TABDEP table. Run the example below. There is a line in the result set for each table that a view is dependent on. For example in the results below, the TRADING.ACCOUNTS view is dependent on on virtual table: INFORMIX.ACCOUNTS. The OHIO query is much more complex and is dependent on a number of views and tables. For example it uses the TRADING.STOCK_HISTORY view which in turn pulls data from the DB2WAREHOUSE.STOCK_HISTORY table. Because of this both dependencies are listed."}, {"metadata": {"id": "1154d34209c2438bb86e2d2ab81a55c4"}, "cell_type": "code", "source": "%%sql -a\nselect tabschema,\n       tabname,\n       bschema as dependent_schema,\n       bname as dependent_name\nfrom syscat.tabdep\nwhere dtype = 'V'\n      and tabschema not like 'SYS%'\n      and tabschema = 'TRADING'\norder by tabschema, tabname, dependent_schema, dependent_name;", "execution_count": null, "outputs": []}, {"metadata": {"id": "ae620c83-2dee-4c71-ba34-13c81c0a828b"}, "cell_type": "markdown", "source": "## Advanced Data Virtualization \nNow that you have seen how powerful and easy it is to gain insight from your existing virtualized data, you can learn more about how to do advanced data virtualization. You will learn how to join different remote tables together to create a new virtual table and how to capture complex SQL into VIEWs.\n\n\n### Joining Tables Together\nThe virtualized tables below come from different data sources on different systems. We can combine them into a single virtual table. \n\n1. Select **My virtualized data** from the Data Virtualization menu\n2. Enter **TRADING** in the find field\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/SearchForTrading.png?raw=true\">\n  \n3. Select the **STOCK_TRANSACTIONS** table\n4. Select the **STOCK_SYMBOLS** table\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/TradingTablesSelected.png?raw=true\">\n  \n5. Click **Join**\n6. In table STOCK_SYMBOLS: deselect **SYMBOL**\n7. In table STOCK_TRANSACTIONS: deselect **TX_NO** \n8. Click **STOCK_TRANSACTION.SYMBOL** and drag to **STOCK_SYMBOLS.SYMBOL**\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.46.50 PM.png?raw=true\">\n \n9. Click **Next**\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.47.46 PM.png?raw=true\">\n  \n10. Check that you can now see both the stock symbol and the full company name. You can also change column names in this page.\n  \n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.48.46 PM.png?raw=true\">\n\n11. Click **Next**\n\n12. Enter **TRANSACTIONS_FULLNAME** into the **Enter view name** field.\n13. Don't change the default schema. This corresponds to your DATAENGINEER user id. \n14. Select **My virtualized data** \n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/MyVirtualizedData.png?raw=true\">\n  \n15. Click **CREATE VIEW**. You see the successful Join View window.\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/SuccessfulJoin.png?raw=true\"> \n  \n16. Click **View my virtualized data**\n17. Click the elipsis menu beside **TRANSACTIONS_FULLNAME**\n18. Click **Preview**. You can confirm that your new join is working.\n18. Click **Back**\n19. Click **View meta data** from the **TRANSACTIONS_FULLNAME** elipsis menu.\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.53.20 PM.png?raw=true\"> \n\n20. Click **Creation SQL**. You can review the statement used to create the view that joins the two tables together.   \n21. Click the **X** at the upper-right of the View screen.\n22. Click **Back**\n\nYou can now join virtualize tables together to combine them into new virtualized tables. Now that you know how to perform simple table joins you can learn how to combine multiple data sources and virtual tables using the powerful SQL query engine that is part of the IBM Cloud Pak for Data - Virtualization."}, {"metadata": {"id": "75407de6-a62e-41c1-87c4-d8a6825ab99c"}, "cell_type": "markdown", "source": "### Using the SQL Editor to Answer Complex Business Questions\nThe IBM Cloud Pak for Data Virtualization Administrator has set up more complex data from multiple source for the next steps. The administrator has also given you access to this virtualized data. You may have noticed this in previous steps. \n1. Select **My virtualized data** from the Data Virtualization menu. All of these virtualized tables look and act like normal Db2 tables. \n2. Click **Preview** for any of the tables to see what they contain.\n3. Select **Run SQL** from the Data Virtualization menu.\n4. Click **Add new script**\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 3.54.26 PM.png?raw=true\">\n  \n5. Click **Open a script to edit** tab\n8. Search for **OHIO Query**\n9. Select and expand the **OHIO QUERY**\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 4.59.25 PM.png?raw=true\">\n\n10. Click the **Open a script to edit** tab open the script in the SQL Editor. \n11. Click **Run All**\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 4.59.50 PM.png?raw=true\">\n\nThis script is a complex SQL join query that uses data from many of the virtual data sources you explored in the first steps of this lab. While the SQL looks complex the author of the query did not have to be aware that the data was coming from multiple sources. Everything used in this query looks like it comes from a single database, not eight different data sources across eight different systems on premises or in the Cloud. "}, {"metadata": {"id": "6ed6e027-556c-4aa7-b0d3-c72e2b525826"}, "cell_type": "markdown", "source": "### Making Complex SQL Simple to Consume\nYou can easily make this complex query easy for a user to consume. Instead of sharing this query with other users, you can wrap the query into a view that looks and acts like a simple table. \n1. Enter **CREATE VIEW MYOHIOQUERY AS** in the SQL Editor at the first line below the comment and before the **WITH** clause\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-16 at 5.00.24 PM.png?raw=true\">\n\n2. Click **Run all**\n3. Click **Add a new script**\n5. Click **Choose script source**\n6. Click **Create new**\n7. Enter **SELECT * FROM MYOHIOQUERY;**\n8. Click **Run all**\n9. Add another line in your script: **SELECT LASTNAME FROM MYOHIOQUERY WHERE TOTAL > 1000;**\n10. Click **Run all**\n\nNow you have a very simple virtualized table that is pulling data from eight different data sources, combining the data together to resolve a complex business problem. In the next step you will share your new virtualized data with a user."}, {"metadata": {"id": "872c1a85-c6e0-4e93-bb23-077af9565fb3"}, "cell_type": "markdown", "source": "### Sharing Virtualized Tables\n1. Select **Virtualization->My virtualized data** from the Data Virtualization Menu.\n2. Click the ellipsis (...) menu to the right of the **MYOHIOQUERY** virtualized table\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/OHIOSearch.png?raw=true\">\n  \n3. Select **Manage Access** from the elipsis menu\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Screen Shot 2020-07-17 at 1.45.18 PM.png?raw=true\">\n \n3. Click **Grant access**\n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/ManageAccessMYOHIOQUERY.png?raw=true\">\n\n4. Search for **User**. \n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/GrantAccessTolabuser.png?raw=true\">\n\n4. Check the box beside **Lab User** and click **Add users**\n    <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/GrantAccessTolabuserCheckbox.png?raw=true\">\n  \n5. Click **Add**\n\nYou should now see that the **Lab User** id has view-only access to the new virtualized table. \n\n  <img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/GrantUserAdded.png?raw=true\">\n\nNext switch to the **Lab User** id to check that you can see the data you have just granted access for.\n\n9. Click the user icon at the very top right of the console\n10. Click **Log out**\n11. Sign in using the **labuser** id with password **tsdvlab**\n12. Click the three bar menu at the top left of the IBM Cloud Pak for Data console\n13. Select **Data Virtualization->My virtualized Data**\n\nYou should see the **MYOHIOQUERY** with the schema from your engineer userid in the list of virtualized data.\n\n14. Make a note of the schema of the MYOHIOQUERY in your list of virtualized tables. It starts with **DATAENGINEER** which corresponds to your username.\n15. Select the **SQL Editor** from the Data virtualization menu\n16. Click **Create new** to open a new SQL Editor window\n17. Enter **SELECT * FROM DATAENGINEERx.MYOHIOQUERY** where x is the user number of your engineer user. The view created by your engineer user was created in their default schema. \n18. Click **Run all**\n19. Add the following to your query: ** WHERE TOTAL > 3000 ORDER BY TOTAL**\n20. Click **</>** to format the query so it is easiler to read\n21. Click **Run all**\n\nYou can see how you have just made a very complex data set extremely easy to consume by a data user. They don't have to know how to connect to multiple data sources or how to combine the data using complex SQL. You can hide that complexity while ensuring only the right user has access to the right data. \n\nIn the next steps you will learn how to access virtualized data from outside of IBM Cloud Pak for Data."}, {"metadata": {"id": "d7ccd1ad-33d0-44a0-bead-34ec602671fa"}, "cell_type": "markdown", "source": "### Allowing User to Access Virtualized Data with Analytic Tools\nIn the next set of steps you connect to virtualized data from this notebook using the **LABUSER** userid. \n\nJust like you connected to IBM Cloud Pak for Data Virtualized Data using your DATAENGINEER user id you can connect using the LABUSER id. The cell below is prefilled since everyone will use the same LABUSER id."}, {"metadata": {"id": "877f0c8d-48f0-42b1-9a9a-ee7ec2b39f8a"}, "cell_type": "markdown", "source": "#### Connecting a USER to Data Virtualization SQL Engine"}, {"metadata": {"id": "afb87583-a8cd-4ac2-8e33-0a115c3382b6"}, "cell_type": "code", "source": "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\ndatabase = 'bigsql'\nuser = 'labuser'\npassword = 'tsdvlab'\nhost = 'cpd-cpd-instance.apps.demo.ibmdte.net'\nport = '31193'\n%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}", "execution_count": null, "outputs": []}, {"metadata": {"id": "fe0c6acf-200f-4e24-9ab4-1bc5144738f5"}, "cell_type": "markdown", "source": "Now you can try out the view that was created by the DATAENGINEER userid. \n\nSubstitute the **x** for the schema used by your ***DATAENGINEERx*** user in the next two cells before you run them."}, {"metadata": {"id": "bfb8e9cb-25e7-4ec2-a1b0-80a47c491d31"}, "cell_type": "code", "source": "%sql SELECT * FROM DATAENGINEERx.MYOHIOQUERY WHERE TOTAL > 3000 ORDER BY TOTAL;", "execution_count": null, "outputs": []}, {"metadata": {"id": "3c8958f5-5c28-4011-981a-694cac362722"}, "cell_type": "markdown", "source": "Only DATAENGINEER virtualized tables that have been authorized for the LABUSER to see are available. Try running the next cell. You should receive an error that the current user does not have the required authorization or privlege to perform the operation."}, {"metadata": {"id": "d3e55ef5-3e86-4167-a91b-49fd99e8b103"}, "cell_type": "code", "source": "%sql SELECT * FROM DATAENGINEERx.DISCOVERFOLD;", "execution_count": null, "outputs": []}, {"metadata": {"id": "079557bc-4785-4ac5-a388-38193610a1b2"}, "cell_type": "markdown", "source": "### Next Steps:\nBefore you start the next section you should log out from the shared lab userid and log back in using your DATAENGINEER id:\n1. Click the user icon at the very top right of the console\n2. Click **Log out**\n3. Sign in using your DATAENGINEERx user id\n4. Click the four-bar menu at the top left of the IBM Cloud Pak for Data console\n5. Select **Data Virtualization**\n\nNow you can use IBM Cloud Pak for Data to make even complex data and queries from different data sources, on premises and across a multi-vendor Cloud look like simple tables in a single database. You are ready for some more advanced labs. \n\n1. Learn how to classify and protect your virtualized data using Watson Knowledge Catalog\n2. Use Open RESTful Services to connect to the IBM Cloud Pak for Data Virtualization \n    * Everything you can do in the IBM Cloud Pak for Data User Interface is accessible through Open RESTful APIs\n    * Learn how to automate and script your managment of Data Virtualization using RESTful API\n    * Learn how to accelerate application development by accessing virtualized data through RESTful APIs"}, {"metadata": {"id": "7fc51219-0326-4644-967e-c3c81bccc8f8"}, "cell_type": "markdown", "source": "## Protecting Virtualized Data using Watson Knowledge Catalog"}, {"metadata": {"id": "57e01129-412b-4a05-b07f-46126f170ef8"}, "cell_type": "markdown", "source": "IBM Watson Knowledge Catalog (included with IBM Cloud Pak for Data) powers intelligent, self-service discovery of data, models and more, activating them for artificial intelligence, machine learning and deep learning. Access, curate, categorize and share data, knowledge assets and their relationships, wherever they reside.\n\nYou can use a Data Governance data protection rule to mask virtual data. When queried, masked columns return disguised data.\n\nData masking applies to the result sets of the queries only. The original data in tables and columns remains untouched. Masking does not apply to query predicates. You can use data masking to avoid exposing sensitive data. However, data masking does not stop Data Virtualization user from connecting to the service and running queries against that data. Users can join and group data, generate the reports, perform analytics and collect insights by using the raw data, while masking the result set only.\n\nMore information is available at: https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_current/cpd/svc/dv/masking.html"}, {"metadata": {"id": "28bf7762e1fd409688eeb7d50e6c5b19"}, "cell_type": "markdown", "source": "### Protecting the Credit Card Numbers in the Virtual Customer Data Tables\nIn this example you explore how to protect data through Data Virtualization and Watson Catalog. The CUSTOMER tables included in this lab environment include a column containing confidential customer credit card data. The lab environment is setup with rules that classify credit card numbers are confidential. While the user who created the virtual table or view can see all the data, other users only see redacted data in the credit card number column."}, {"metadata": {"id": "89d8582e-ab8f-4c95-8f87-17f987a97501"}, "cell_type": "markdown", "source": "### Enable Policy Enforcement of Virtualized Data\nBefore you can establish rules to protect virtualized data, the Cloud Pak for Data Administrator must enable policy enforcement and data visibility restrictions in the Data Virtualization Settings. As a Data Engineer user you do not have access to Data Virtualization settings. You need Cloud Pak for Data Administrative authority. To make the changes on your own cluster: Select **Service Settings** from the Data Virtualization menu and select the **Governance** tab. \n\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/ServiceSettings.png?raw=true\">\n\nYou can then ensure that **Governance policies** and **Publish to catalog** are both set to **Enforced**.\n\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/ServiceSettingsGovernance.png?raw=true\">"}, {"metadata": {"id": "a0990678-4b0d-4010-a2ad-7267a41f9f5d"}, "cell_type": "markdown", "source": "### Request to Publish Virtualized Tables to Watson Knowledge Catalog\nTo protect virtualized data you need to add the virtualized table to the Watson Knowledge Catalog. Since we have one rule already in place any of the virtual tables you create were automatically added to the catalog. \n\nYou may remember seeing the following message when you created your virtual tables:\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Virtualize7.png?raw=true\">"}, {"metadata": {"id": "c682b7b3-0297-4aeb-8e5d-113f385eed81"}, "cell_type": "markdown", "source": "### Approving Publish Request\nIn an environment without any data rules, a user with Data Steward authority needs to approve the request to add the CUSTOMER_PAYMENT table in your DATAENGINEER schema to the catalog. In this lab that happened automatically to ensure that no data escapes the rules. "}, {"metadata": {"id": "7ff53cb6-edbb-4990-8a1f-d2fb9ec15525"}, "cell_type": "markdown", "source": "### Check and Classify your Virtualized Data\nOnce data has been added to the Watson Knowledge catalog you can review the data and classify it.\n1. Select **Catalogs** and **All catalogs** from the main Cloud Pak for Data menu\n2. Click **Default Catalog**\n3. Enter **CUSTOMER** into the search bar\n4. Click the **DB2OLTPONCPD.CUSTOMER** table\n5. Click the **Profile** tab. You may need to log in again as the DATAENGINEERnn. Notice the drop down selection available below each column title. This is the data classification. While Waston Knowledge Catalog will attempt to classify new columns in new tables you may have to manually classify data. In this example the table was added by the administrator userid and the CARD_NO column was manually classified as Credit Card Data.\n6. Scroll to the right to check the CARD_NO column. If profiling is complete you should see details on all the columns except for CARD_NO. Notice that the Credit Card Number column is unavailable because the data in the column is anonymized.\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/CUSTOMERProfile.png?raw=true\">"}, {"metadata": {"id": "c494208d-d8f7-416f-a4bc-36fdc860d53f"}, "cell_type": "markdown", "source": "### Review the existing Data Protection Rules\nNow we can check the rules that are already in place that control how users can access our virtualized data.\n1. Select **Governace** and **Rules** from the main Cloud Pak for Data menu\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/Rules.png?raw=true\">\n2. Click **Mask Credit Card Numbers**\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/MaskCreditCardNumbers.png?raw=true\">\n3. Click **Edit**. While, you are not going to make any changes to the rule, you can review the options.\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/RuleEdit.png?raw=true\">\n4. Click **Cancel**\n\nYou can find out more about Data Protection Rules in the Cloud Pak for Data Knowledge Center: https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_current/wsj/governance/dmg_rules.html"}, {"metadata": {"id": "16dca2ed-6527-48ce-93a4-e110e9f0571a"}, "cell_type": "markdown", "source": "### Preview Data in the Cloud Pak for Data Console\nOnce you data has been classified and profiled protection is in place. Following the established protection rule, the credit card number will be redacted (replaced with Xs) everywhere it is accessible through Cloud Pak for Data. As you saw in the last step, it is reacted when you, and other users, interact with it through Watson Knowledge Catalog. \n\nIt is also redacted if you access the data through the Data Virtualization console. \n\nNext, preview the data you just reviewed:\n1. Select **Data** and **Data virtualization** from the main Cloud Pak for Data menu\n2. Select **My virtualized data** from the Data virtualization menu\n3. Enter **CUSTOMER** in the **Find virtual objects** search\n4. Click the **DB2OLTPONCPD.CUSTOMER** table\n5. Click the ellipsis menu to the right of that row\n6. Select **Preview**. The CARD_NO column should be redacted with Xs.\n<img src=\"https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/media/MaskedVirtualTable.png?raw=true\">"}, {"metadata": {"id": "820e29c7-a747-46d3-a2b4-c3fc649f96fb"}, "cell_type": "markdown", "source": "### Test Data Protection\nImportantly, the Credit Card information is protected when the data is access from an outside application. Let's reconnect to the Data Virtualization Engine as your DATAENGINEER user."}, {"metadata": {"id": "33b38c927dce4c3b85eb4782ffcc9d21"}, "cell_type": "code", "source": "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\ndatabase = 'bigsql'\nuser = engineer\npassword = 'tsdvlab'\nhost = 'cpd-cpd-instance.apps.demo.ibmdte.net'\nport = '31193'\n\n%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}", "execution_count": null, "outputs": []}, {"metadata": {"id": "8357dc119f2846729ba838754b37dd80"}, "cell_type": "markdown", "source": "Now we can run a test query"}, {"metadata": {"id": "c7d3d371-3ebb-469a-aa7b-6a21f5c8bd0e"}, "cell_type": "code", "source": "%sql SELECT * FROM DB2OLTPONCPD.CUSTOMER FETCH FIRST 10 ROWS ONLY;", "execution_count": null, "outputs": []}, {"metadata": {"id": "ea8ff1c5a79f4f26892853df8e60b28f"}, "cell_type": "markdown", "source": "You should see the CARD_NO column results only return Xs. Any application that accesses this data through the Data Virtualization engine will be redacted in the same way. Remember you are still connected through Python as the USER id."}, {"metadata": {"id": "4c09cb78-3674-4f3e-b8d6-597945ec5bb8"}, "cell_type": "markdown", "source": "## Automating Data Virtualization Setup and Management through REST"}, {"metadata": {"id": "73d2e8f8-7e29-4064-8660-55157d988386"}, "cell_type": "markdown", "source": "The IBM Cloud Pak for Data Console is only one way you can interact with the Virtualization service. IBM Cloud Pak for Data is built on a set of microservices that communicate with each other and the Console user interface using RESTful APIs. You can use these services to automate anything you can do through the user interface.\n\nThis Jupyter Notebook contains examples of how to use the Open APIs to retrieve information from the virtualization service, how to run SQL statements directly against the service through REST and how to provide authorization to objects. This provides a way write your own script to automate the setup and configuration of the virtualization service. "}, {"metadata": {"hide_input": true, "id": "2431cfb8-f955-43fd-b2ff-744dce710666"}, "cell_type": "markdown", "source": "The next part of the lab relies on a set of base classes to help you interact with the RESTful Services API for IBM Cloud Pak for Data Virtualization. You can access this library on GITHUB. The commands below download the library and run them as part of this notebook.\n<pre>\n&#37;run CPDDVRestClassV402.ipynb\n</pre>\nThe cell below loads the RESTful Service Classes and methods directly from GITHUB. Note that it will take a few seconds for the extension to load, so you should generally wait until the \"Db2 Extensions Loaded\" message is displayed in your notebook. You can click on the following like to browse the RESTful Services class file: https://github.com/Db2-DTE-POC/CPDDVHOL4/blob/main/RESTfulEndpointServiceClass402.ipynb. You are free to download and reuse this sample for your own applications.\n\n1. Click the cell below\n2. Click **Run**"}, {"metadata": {"id": "462bf045-b249-446c-b5ed-ff04b8dd097a"}, "cell_type": "code", "source": "!wget -O CPDDVRestClassV402.ipynb https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVHOL4/main/CPDDVRestAPIClass402.ipynb\n%run CPDDVRestClassV402.ipynb", "execution_count": null, "outputs": []}, {"metadata": {"id": "7cd34039-b740-4fed-883a-8162c57477ed"}, "cell_type": "markdown", "source": "### The DV REST Class\nThe CPDDVRestClassV402.ipynb notebook includes a Python class called DVREST that encapsulates the Rest API calls used to connect to the IBM Cloud Pak for Data Virtualization service. \n\nTo access the service you need to first authenticate with the service and create a reusable token that we can use for each call to the service. This ensures that we don't have to provide a userID and password each time we run a command. The token makes sure this is secure. \n\nEach request is constructed of several parts. First, the URL and the API identify how to connect to the service. Second the REST service request that identifies the request and the options. For example '/metrics/applications/connections/current/list'. And finally some complex requests also include a JSON payload. For example running SQL includes a JSON object that identifies the script, statement delimiters, the maximum number of rows in the results set as well as what do if a statement fails."}, {"metadata": {"id": "a171703e-115c-411a-9e77-14465de0a15f"}, "cell_type": "markdown", "source": "### Example Connections\nTo connect to the Data Virtualization service you need to provide the URL, the service name (v1) and profile the console user name and password. \n\n1. Substitute your assigned DATAENGINEER userid below along with your password you used to log into IBM Cloud Pak for Data at the beginning of the lab. \n2. Run the next cell. \n\nThe cell generates a bearer token that is used in the following steps to authenticate your use of the API. "}, {"metadata": {"id": "beac4bf1-d67c-4f70-a882-ecfca55c0f9d"}, "cell_type": "markdown", "source": "#### Connecting to Data Virtualization API Service"}, {"metadata": {"id": "3ba1bb3e-5658-4f6a-8f15-8dd8a67ee735"}, "cell_type": "code", "source": "# Set the service URL to connect from inside the ICPD Cluster\nConsole  = 'https://cpd-cpd-instance.apps.demo.ibmdte.net:31192'\n\n# Connect to the Db2 Data Management Console service\nuser     = engineer\npassword = 'tsdvlab'\n\n# Set up the required connection\ndatabaseAPI = DVRESTAPI(Console)\napi = '/v1'\ndatabaseAPI.authenticate(api, user, password)\ndatabase = Console", "execution_count": null, "outputs": []}, {"metadata": {"id": "19924a84-3deb-4879-a24f-d206e01fbb40"}, "cell_type": "markdown", "source": "#### Data Sources and Availability\nThe following Python function (getDataSources) runs SQL against the **QPLEXSYS.LISTRDB** catalog table and combines it with a stored procedure call **QPLEXSYS.LISTRDBCDETAILS()** to add the **AVAILABLE** column to the results. The IBM Cloud Pak for Data Virtualization Service checks each data sources every 5 to 10 seconds to ensure that it is still up and available. In the table (DataFrame) in the next cell a **1** in the **AVAILABLE** column indicates that the data source is responding. A **0** indicdates that it is not longer responding. \n\nRun the following cell."}, {"metadata": {"id": "1e4b21fe-a85c-464a-97b5-23b27d26f07b"}, "cell_type": "code", "source": "# Display the Available Data Sources already configured\n\ndataSources = databaseAPI.getDataSources()\ndisplay(dataSources)", "execution_count": null, "outputs": []}, {"metadata": {"id": "0e303da6-6ca2-4fea-b9ac-dbc7bb8a3ce9"}, "cell_type": "markdown", "source": "#### Virtualized Data\nThis call retrieves all of the virtualized data available to the role of Data Engineer. It uses a direct RESTful service call and does not use SQL. The service returns a JSON result set that is converted into a Python Pandas dataframe. Dataframes are very useful in being able to manipulate tables of data in Python. If there is a problem with the call, the error code is displayed."}, {"metadata": {"id": "78453604-a7e6-4d1d-8ac8-16dc5627f041"}, "cell_type": "code", "source": "# Display the Virtualized Assets Avalable to Engineers and Users\nroles = ['DV_ENGINEER']\nfor role in roles:\n    r = databaseAPI.getRole(role)\n    if (databaseAPI.getStatusCode(r)==200):\n        json = databaseAPI.getJSON(r)\n        df = pd.DataFrame(json_normalize(json['objects']))\n        display(df)\n    else:\n        print(databaseAPI.getStatusCode(r))  ", "execution_count": null, "outputs": []}, {"metadata": {"id": "039f0b94-cd74-4429-89c3-f19047eb1473"}, "cell_type": "markdown", "source": "#### Virtualized Tables and Views\nThis call retrieves all the virtualized tables and views available to the userid that you use to connect to the service. In this example the whole call is included in the DVRESTAPI class library and returned as a complete Dataframe ready for display or to be used for analysis or administration."}, {"metadata": {"id": "d46ec001-07e4-4240-bd00-5e87733e9ff4"}, "cell_type": "code", "source": "### Display Virtualized Tables \ndisplay(databaseAPI.getVirtualizedTablesDF())", "execution_count": null, "outputs": []}, {"metadata": {"id": "570664a38ca54de681de1601ff0e9d38"}, "cell_type": "code", "source": "### Display Views \ndisplay(databaseAPI.getVirtualizedViewsDF())", "execution_count": null, "outputs": []}, {"metadata": {"id": "60891cad-343d-46a2-9228-cbd65b0fb64e"}, "cell_type": "markdown", "source": "#### Get a list of the IBM Cloud Pak for Data Users\nThis example returns a list of all the users of the IBM Cloud Pak for Data system. It only displays three colunns in the Dataframe, but the list of all the available columns is als printed out. Try changing the code to display other columns."}, {"metadata": {"id": "95689419-b2ee-40b3-adf2-cac9c78cedd7"}, "cell_type": "code", "source": "# Get the list of Cloud Pak for Data Users\ndisplay(databaseAPI.getUsersDF()[['uid','username','displayName','user_roles','role']])", "execution_count": null, "outputs": []}, {"metadata": {"id": "f2b2b2c5-a667-4bfd-a380-e6222ea51baa"}, "cell_type": "markdown", "source": "#### Get the list of enabled Data Virtualization Caches\nThis operation is only available to Cloud Pak for Data and Data Virtualization Administrators. So you will receive an error message. Using the API calls is not a back door past basic security. "}, {"metadata": {"id": "d8a7f6fc32b44f0a805f32a7a56ed767"}, "cell_type": "code", "source": "#Get the list of active Data Virtualization Caches, only available for adminstrators\ndisplay(databaseAPI.getCaches())", "execution_count": null, "outputs": []}, {"metadata": {"id": "a9182fcd-8404-4bea-a6f1-9354e95914ef"}, "cell_type": "markdown", "source": "### What's next\nYou can download a copy of your completed Jupyter notebook as a reference:\n1. Click **File** from the Jupyter notebook main menu\n2. Select **Download as**\n3. Select **Notebook** if you want to use this notebook in your own Jupyter environment\n4. Select **HTML** if you want a read only version of the notebook for reference\n\nCheck out the Cloud Pak for Data Advanced Administrator Hands on Labs and Demonstrations in the Advanced Hands-on Labs Project. "}, {"metadata": {"id": "1090abff-db1f-4abc-8bd6-7d1023fcc370"}, "cell_type": "markdown", "source": "#### Credits: IBM 2021-2022, Peter Kohlmann [kohlmann@ca.ibm.com]"}, {"metadata": {"id": "7fec0a46ccf14e1487fb2988b7630256"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.11", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}